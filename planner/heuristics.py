import sys
import util
from node import Node
from state import State

def applicable(state, actions):
    ''' Return a list of applicable actions in a given `state`. '''
    app = list()
    for act in actions:
        if State(state).intersect(act.precond) == act.precond:
            app.append(act)
    return app

def successorRelaxed(state, action):
    ''' Return the sucessor state generated by executing `action` in `state`. '''
    return State(action.pos_effect).union(state)

def layerGoals(state, predicates):
    return State(state).union(predicates)

def goal_test(state, goal):
    ''' Return true if `state` is a goal state. '''
    return State(state).intersect(goal) == State(goal)

def h_naive(state, planning):
    return 0

def h_ff(state, planning, goal):
    graphplan = dict() #graphplan relaxed
    actions = planning.actions
    #goal = planning._problem.goal
    X = state
    isGoal = False
    if X.intersect(goal) == goal: #ja estamos na meta entao o compimento (a quantidade) de acoes necessaria eh zero
        return 0
    level = 0
    graphplan[(level,'state')] = X
    #PHASE 1 - expand graph
    while not isGoal:
        actionsApplicable = applicable(X,actions)
        level += 1
        for a in actionsApplicable:
            X = successorRelaxed(X,a) #added positive effects of a
            if X.intersect(goal) == goal:
                isGoal = True
                break
        graphplan[(level,'state')] = X
        graphplan[(level,'action')] = actionsApplicable
    #PHASE 2 - busca regressiva - partindo dos atomos do goal ate termos os atomos do state
    thisLevelGoals = set()
    thisLevelGoals = thisLevelGoals.union(goal)
    relaxedActions = set()
    while (level > 0):
        prevLevelGoals = set()
        for tg in thisLevelGoals:
            if tg in graphplan[level-1,'state']:
                prevLevelGoals.add(tg)
            else:
                for a in graphplan[level,'action']:
                    if tg in a.pos_effect:
                        prevLevelGoals = prevLevelGoals.union(a.precond)
                        relaxedActions.add(a)
                        break 
        level -= 1
        thisLevelGoals = prevLevelGoals.copy()
    return len(relaxedActions)

def h_add_planner(state, planning, goal):
    h = dict() 
    actions = planning.actions
    X = state
    for x in X:
        h[x] = 0
    change = True
    while change:
        change = False
        actionsApplicable = applicable(X,actions)
        for a in actionsApplicable:
            X = successorRelaxed(X,a) #added positive effects of a
            for p in a.pos_effect:
                prev = h.get(p,sys.maxsize)
                h[p] = min(prev,(1+sum(h.get(pre, sys.maxsize) for pre in a.precond)))
                if prev != h[p]:
                    change = True
    return sum(h.get(i,sys.maxsize) for i in goal)

# heuristica usada nos testes brucutu e local...estava dando bons resultados 
# def h_add(planning, state):
#     h = dict() 
#     actions = planning.actions
#     init = planning.problem.init
#     X = init
#     for x in X:
#         h[x] = 0
#     change = True
#     while change:
#         change = False
#         actionsApplicable = applicable(X,actions)
#         for a in actionsApplicable:
#             X = successorRelaxed(X,a) #added positive effects of a
#             for p in a.pos_effect:
#                 prev = h.get(p,sys.maxsize)
#                 h[p] = min(prev,(1+sum(h.get(pre, sys.maxsize) for pre in a.precond)))
#                 if prev != h[p]:
#                     change = True
#     '''
#     selecting only atoms that belongs to state
#     node_dict = {p:h.get(p,sys.maxsize) for p in state} 
#     return node_dict
#     '''
#     return h
#     #return sum(h.get(i,sys.maxsize) for i in goal)

def h_add(planning, state):
    h = dict() 
    actions = planning.actions
    #init = planning.problem.init
    X = state
    for x in X:
        h[x] = 0
    change = True
    while change:
        change = False
        actionsApplicable = applicable(X,actions)
        for a in actionsApplicable:
            X = successorRelaxed(X,a) #added positive effects of a
            for p in a.pos_effect:
                prev = h.get(p,sys.maxsize)
                h[p] = min(prev,(1+sum(h.get(pre, sys.maxsize) for pre in a.precond)))
                if prev != h[p]:
                    change = True
    '''
    selecting only atoms that belongs to state
    node_dict = {p:h.get(p,sys.maxsize) for p in state} 
    return node_dict
    '''
    return h
    #return sum(h.get(i,sys.maxsize) for i in goal)
